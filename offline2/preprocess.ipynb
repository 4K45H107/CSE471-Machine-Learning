{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, numerical_scaling='minmax'):\n",
    "        self.numerical_scaling = numerical_scaling\n",
    "\n",
    "\n",
    "    # PREPROCESSING\n",
    "    # ---------------------\n",
    "    def preprocess(self, df):\n",
    "\n",
    "        df = self._handle_missing_values(df)\n",
    "        df = self._handle_features(df)\n",
    "\n",
    "        boolean_cols = df.select_dtypes(include='bool').columns\n",
    "        df[boolean_cols] = df[boolean_cols].astype(int)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    # HANDLING - MISSING VALUES\n",
    "    # --------------------------------\n",
    "    def _handle_missing_values(self, df):\n",
    "        df = self.handle_missing_numerical(df)\n",
    "        df = self.handle_missing_categorical(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Numerical\n",
    "    def handle_missing_numerical(self, df):\n",
    "        mean_col = df.select_dtypes(include='number').mean()\n",
    "        df = df.fillna(mean_col)\n",
    "        return df\n",
    "\n",
    "    # Categorical\n",
    "    def handle_missing_categorical(self, df):\n",
    "        mode_col = df.select_dtypes(include='object').mode() \n",
    "        for col in mode_col.columns:\n",
    "            df[col] = df[col].fillna(mode_col[col].iloc[0])\n",
    "            \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    # HANDLING - FEATURES\n",
    "    # ----------------------------\n",
    "    def _handle_features(self, features):\n",
    "        features = self.handle_categorical_features(features)        \n",
    "        features = self.handle_numerical_features(features)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    # Categorical\n",
    "    def handle_categorical_features(self, features):\n",
    "        for column in features.columns:\n",
    "            if features[column].dtype == 'object':    \n",
    "                values = features[column].unique()\n",
    "\n",
    "                if (len(values) > 2):\n",
    "                    features = pd.get_dummies(features, columns=[column])\n",
    "                elif (len(values) == 2):\n",
    "                    features[column] = features[column].map({values[0]: 0, values[1]: 1})\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Numerical\n",
    "    def handle_numerical_features(self, features):\n",
    "        if self.numerical_scaling == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif self.numerical_scaling == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "\n",
    "        numeric_cols = features.select_dtypes(include=['number']).columns\n",
    "        features[numeric_cols] = scaler.fit_transform(features[numeric_cols])\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_one():\n",
    "    data = pd.read_csv(\"./1st/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "    # Initial data cleaning\n",
    "    df_filtered = data.drop(columns=[\"customerID\"])\n",
    "    df_filtered.dropna(subset=['Churn'], inplace=True)\n",
    "\n",
    "    # Splitting the data into features and target\n",
    "    features = df_filtered.drop(columns=[\"Churn\"])\n",
    "    target = data[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    # handling special cases\n",
    "    features[\"TotalCharges\"] = pd.to_numeric(features[\"TotalCharges\"], errors='coerce')\n",
    "\n",
    "    # preprocessing\n",
    "    preprocessor = Preprocessor()\n",
    "    features = preprocessor.preprocess(features)\n",
    "\n",
    "    # saving the preprocessed dataset\n",
    "    preprocessed_dataset = pd.concat([features, target], axis=1) \n",
    "    preprocessed_dataset = preprocessed_dataset.rename(columns={\"Churn\": \"label\"})\n",
    "\n",
    "    preprocessed_dataset.to_csv(\"./dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_three():\n",
    "    data = pd.read_csv(\"./3rd/creditcard.csv\")\n",
    "\n",
    "    # counts = data[\"Class\"].value_counts()\n",
    "    # print(counts)\n",
    "    # counts -> 0: 284315, 1: 492\n",
    "    # high class imbalance  \n",
    "\n",
    "    #$initaial data cleaning\n",
    "    df_filtered = data.dropna(subset=['Class'])\n",
    "\n",
    "    # Splitting the data into features and target\n",
    "    features = df_filtered.drop(\"Class\", axis=1)\n",
    "    target = df_filtered[\"Class\"]\n",
    "\n",
    "    # preprocessing\n",
    "    preprocessor = Preprocessor()\n",
    "    features = preprocessor.preprocess(features)\n",
    "\n",
    "    # saving the preprocessed dataset\n",
    "    preprocessed_dataset = pd.concat([features, target], axis=1) \n",
    "    preprocessed_dataset = preprocessed_dataset.rename(columns={\"Class\": \"label\"})\n",
    "\n",
    "    # EXTRA PROCESS\n",
    "    # taking a portion of data to balance the classes and efficiency\n",
    "    zeros_df = preprocessed_dataset[preprocessed_dataset['label'] == 0].sample(n=20000, random_state=42)  \n",
    "    ones_df = preprocessed_dataset[preprocessed_dataset['label'] == 1] \n",
    "\n",
    "    combined_df = pd.concat([zeros_df, ones_df])\n",
    "    combined_df = combined_df.sample(frac=1, random_state=42)\n",
    "\n",
    "    # print(combined_df[\"label\"].value_counts())\n",
    "\n",
    "    combined_df.to_csv(\"./dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_two():\n",
    "\n",
    "    #  .DATA file\n",
    "    # -----------------\n",
    "    # 39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "    data = pd.read_csv(\"./2nd/adult.data\", header=None, names=column_names)\n",
    "    # print(data.shape)\n",
    "\n",
    "    # handling special cases\n",
    "    for column in data.columns:\n",
    "        values = data[column].unique()\n",
    "        if data[column].dtype.name == 'object':\n",
    "            if ('?' in values or ' ?' in values):\n",
    "                data[column] = data[column].replace('?', None)\n",
    "                data[column] = data[column].replace(' ?', None)\n",
    "\n",
    "    # Initial data cleaning\n",
    "    df_filtered = data.dropna(subset=['income'])\n",
    "    # print(data['native_country'].nunique())\n",
    "\n",
    "    # Splitting the data into features and target\n",
    "    features = df_filtered.drop('income', axis=1)\n",
    "    target = df_filtered['income']\n",
    "\n",
    "    # handling target\n",
    "    target = target.map({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "    \n",
    "    # preprocessing\n",
    "    preprocessor = Preprocessor()\n",
    "    features = preprocessor.preprocess(features)\n",
    "\n",
    "    # saving the preprocessed dataset\n",
    "    preprocessed_dataset = pd.concat([features, target], axis=1) \n",
    "    preprocessed_dataset = preprocessed_dataset.rename(columns={\"income\": \"label\"})\n",
    "\n",
    "    # print(preprocessed_dataset.shape)\n",
    "\n",
    "    preprocessed_dataset.to_csv(\"./dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "    # .TEST file\n",
    "    # ----------------- \n",
    "\n",
    "    data_test = pd.read_csv(\"./2nd/adult.test\", header=None, names=column_names)\n",
    "\n",
    "    # handling special cases\n",
    "    for column in data_test.columns:\n",
    "        values = data_test[column].unique()\n",
    "        if data_test[column].dtype.name == 'object':\n",
    "            if ('?' in values or ' ?' in values):\n",
    "                data_test[column] = data_test[column].replace('?', None)\n",
    "                data_test[column] = data_test[column].replace(' ?', None)\n",
    "    \n",
    "    # Initial data cleaning\n",
    "    df_filtered_test = data_test.dropna(subset=['income'])\n",
    "\n",
    "    # Splitting the data into features and target\n",
    "    features_test = df_filtered_test.drop('income', axis=1)\n",
    "    target_test = df_filtered_test['income']\n",
    "\n",
    "    # handling special cases\n",
    "    target_test = target_test.map({' <=50K.': 0, ' >50K.': 1})\n",
    "\n",
    "    # preprocessing\n",
    "    preprocessor_test = Preprocessor()\n",
    "    features_test = preprocessor_test.preprocess(features_test)\n",
    "\n",
    "    missing_cols = list(set(features.columns) - set(features_test.columns))\n",
    "    # print(missing_cols)\n",
    "\n",
    "\n",
    "    missing_df = pd.DataFrame(0, index=features_test.index, columns=missing_cols)\n",
    "    features_test = pd.concat([features_test, missing_df], axis=1)\n",
    "\n",
    "    # print(features_test.columns)\n",
    "    features_test = features_test[features.columns]\n",
    "\n",
    "    # saving the preprocessed dataset\n",
    "    preprocessed_dataset_test = pd.concat([features_test, target_test], axis=1) \n",
    "    preprocessed_dataset_test = preprocessed_dataset_test.rename(columns={\"income\": \"label\"})\n",
    "\n",
    "    # print(preprocessed_dataset_test.shape)\n",
    "\n",
    "    preprocessed_dataset_test.to_csv(\"./dataset_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 105)\n",
      "(16281, 105)\n"
     ]
    }
   ],
   "source": [
    "# dataset_one()\n",
    "dataset_two()\n",
    "# dataset_three()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
